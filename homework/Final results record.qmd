---
title: "Final results(record)"
format: html
---

# Results from the Navie Bayes Model

- Using optimal feature set from the previous section and fitting a final “optimal” NB model, we get the following results. 

![result](./images/result-1.png)

# Evaluation

- **Accuracy:** the accuracy of the model is 63.81%, which means that the classifier correctly predicted the adaptivity level for approximately 64 out of every 100 students.

- **Precision:** the precision for the classes 0, 1, and 2 are 0.49, 0.66, and 0.65, respectively. This means that when the model predicts a student's adaptivity level as class 0, it is correct about 49% of the time. For class 1, it is correct 66% of the time, and for class 2, 65% of the time.

- **Recall:** the recall for the classes 0, 1, and 2 are 0.46, 0.71, and 0.61, respectively. This implies that the model identifies 46% of all actual class 0 adaptivity levels correctly, 71% of class 1, and 61% of class 2.

- **F1-scores:** the F1-scores for the classes 0, 1, and 2 are 0.47, 0.69, and 0.63, respectively. The F1-score balances the trade-off between precision and recall. For class 1, the balance is better (F1-score 0.69) compared to class 0 (F1-score 0.47) and class 2 (F1-score 0.63), indicating a more reliable performance for class 1 predictions.

- **Support:** There were 39 instances of class 0, 154 of class 1, and 169 of class 2 in the test set. This could suggest why the classifier's performance for class 0 might not be as high as for the other classes, as there are fewer examples to learn from.



# Discussion

- The data is split such that a certain percentage ( 70% for training and 30% for testing). The Naive Bayes classifier was trained on the training dataset. During this phase, the model learns the relationship between the features of the input data and the target variable. It does this by analyzing the distribution of the features and computing the conditional probability of each class given an input.

- When we divide our data into training and testing sets, it's important to note that the model doesn't have any prior knowledge of the testing set. The testing set comprises both the features (independent variables) and the actual labels (the ground truth we aim to predict). To evaluate the model's performance, we utilize a trained Naive Bayes model and apply it to predict the labels for the testing data. This involves feeding the testing set's features into the model's prediction function. The model then uses the probability information it acquired during training to determine the most probable label for each instance within the testing set. The predictions generated by the model for the testing set are subsequently compared against the actual labels of the testing set. This comparison serves as the test of the model's predictive accuracy.

# Visualizations of the results

- The confusion matrix below shows the true labels vs. the predicted labels for each class. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while the off-diagonal elements are those that are mislabeled by the classifier.

![confusion matrix](./images/confusion.png)


-  The training score (blue line) is the accuracy of the model on the training set. A high score indicates that the model performs well on the training data. The cross-validation score (red line) is the accuracy of the model on the validation set. A high score indicates that the model generalizes well to unseen data. The shaded areas represent the variability (standard deviation) of the scores around the mean.

- As the number of training samples increases, the training score decreases, which is typical because models tend to perform better when they are trained on fewer samples (they can memorize the training data). The cross-validation score generally increases with more data, which indicates that the model is generalizing better with more data. The convergence of the training and validation scores suggests that adding more training samples may not lead to significant improvements in the model's performance. This can indicate that the model has reached its performance limit given the current features and model complexity.

![learning curve](./images/learning.png)

- A validation curve is a tool used to improve the performance of a machine learning model by tuning hyperparameters. It shows the relationship between a single hyperparameter value and the model's performance, which is measured by some scoring metric (e.g., accuracy). By observing how the training score and the cross-validation score change with different values of a hyperparameter, we can choose the best value that will hopefully generalize well to unseen data.

- For the Naive Bayes classifier we've been using, a key hyperparameter to vary could be the var_smoothing parameter of the GaussianNB classifier, which is used to specify the portion of the largest variance of all features that is added to variances for calculation stability.


- From the plot, we can observe how the accuracy of the model changes with different values of var_smoothing. Ideally, we're looking for a value where the cross-validation score is high, and the gap between the training and cross-validation scores is relatively small, indicating that the model generalizes well without overfitting.

![Validation Curve](./images/Validation.png)


- *Note: documentation process is facilitated by Chatgpt*
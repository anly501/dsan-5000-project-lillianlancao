[
  {
    "objectID": "Data Preparation.html",
    "href": "Data Preparation.html",
    "title": "Data Preparation",
    "section": "",
    "text": "Why splitting our dataset?\nProperly splitting the dataset is fundamental to building a robust and generalizable machine learning model. The training set allows the model to learn, the validation set helps in tuning and selecting the best model, and the testing set provides an unbiased evaluation of the final model’s performance.\n\n\nHow to split our dataset?\nStratified Split: Given that the target variable is “Adaptivity Level” (which is categorical), we might want to ensure that the distribution of classes in the target variable is preserved in both the training and testing sets. This is especially important if the classes are imbalanced. A stratified split would help in maintaining the class distribution."
  },
  {
    "objectID": "Final results record.html",
    "href": "Final results record.html",
    "title": "Final results(record)",
    "section": "",
    "text": "Results from the Navie Bayes Model\n\nUsing optimal feature set from the previous section and fitting a final “optimal” NB model, we get the following results.\n\n\n\n\nresult\n\n\n\n\nEvaluation\n\nAccuracy: the accuracy of the model is 63.81%, which means that the classifier correctly predicted the adaptivity level for approximately 64 out of every 100 students.\nPrecision: the precision for the classes 0, 1, and 2 are 0.49, 0.66, and 0.65, respectively. This means that when the model predicts a student’s adaptivity level as class 0, it is correct about 49% of the time. For class 1, it is correct 66% of the time, and for class 2, 65% of the time.\nRecall: the recall for the classes 0, 1, and 2 are 0.46, 0.71, and 0.61, respectively. This implies that the model identifies 46% of all actual class 0 adaptivity levels correctly, 71% of class 1, and 61% of class 2.\nF1-scores: the F1-scores for the classes 0, 1, and 2 are 0.47, 0.69, and 0.63, respectively. The F1-score balances the trade-off between precision and recall. For class 1, the balance is better (F1-score 0.69) compared to class 0 (F1-score 0.47) and class 2 (F1-score 0.63), indicating a more reliable performance for class 1 predictions.\nSupport: There were 39 instances of class 0, 154 of class 1, and 169 of class 2 in the test set. This could suggest why the classifier’s performance for class 0 might not be as high as for the other classes, as there are fewer examples to learn from.\n\n\n\nDiscussion\n\nThe data is split such that a certain percentage ( 70% for training and 30% for testing). The Naive Bayes classifier was trained on the training dataset. During this phase, the model learns the relationship between the features of the input data and the target variable. It does this by analyzing the distribution of the features and computing the conditional probability of each class given an input.\nWhen we divide our data into training and testing sets, it’s important to note that the model doesn’t have any prior knowledge of the testing set. The testing set comprises both the features (independent variables) and the actual labels (the ground truth we aim to predict). To evaluate the model’s performance, we utilize a trained Naive Bayes model and apply it to predict the labels for the testing data. This involves feeding the testing set’s features into the model’s prediction function. The model then uses the probability information it acquired during training to determine the most probable label for each instance within the testing set. The predictions generated by the model for the testing set are subsequently compared against the actual labels of the testing set. This comparison serves as the test of the model’s predictive accuracy.\n\n\n\nVisualizations of the results\n\nThe confusion matrix below shows the true labels vs. the predicted labels for each class. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while the off-diagonal elements are those that are mislabeled by the classifier.\n\n\n\n\nconfusion matrix\n\n\n\nThe training score (blue line) is the accuracy of the model on the training set. A high score indicates that the model performs well on the training data. The cross-validation score (red line) is the accuracy of the model on the validation set. A high score indicates that the model generalizes well to unseen data. The shaded areas represent the variability (standard deviation) of the scores around the mean.\nAs the number of training samples increases, the training score decreases, which is typical because models tend to perform better when they are trained on fewer samples (they can memorize the training data). The cross-validation score generally increases with more data, which indicates that the model is generalizing better with more data. The convergence of the training and validation scores suggests that adding more training samples may not lead to significant improvements in the model’s performance. This can indicate that the model has reached its performance limit given the current features and model complexity.\n\n\n\n\nlearning curve\n\n\n\nA validation curve is a tool used to improve the performance of a machine learning model by tuning hyperparameters. It shows the relationship between a single hyperparameter value and the model’s performance, which is measured by some scoring metric (e.g., accuracy). By observing how the training score and the cross-validation score change with different values of a hyperparameter, we can choose the best value that will hopefully generalize well to unseen data.\nFor the Naive Bayes classifier we’ve been using, a key hyperparameter to vary could be the var_smoothing parameter of the GaussianNB classifier, which is used to specify the portion of the largest variance of all features that is added to variances for calculation stability.\nFrom the plot, we can observe how the accuracy of the model changes with different values of var_smoothing. Ideally, we’re looking for a value where the cross-validation score is high, and the gap between the training and cross-validation scores is relatively small, indicating that the model generalizes well without overfitting.\n\n\n\n\nValidation Curve\n\n\n\nNote: documentation process is facilitated by Chatgpt"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Social Media Analysis for Education Trends\nAuthor: Lan Cao\n\n\nBackground\nTechnology has reshaped different aspects of our life, and one of them is education. Covid-19 made online courses become an indispensable part of our daily study. Online learning platforms like Coursera and Udemy are proving people with tons of opportunities of absorbing knowledge from many industries. Artificial Intelligence has also been applied to helping students improving their reading, writing and speaking. For example, Grammarly makes use of NLP(natural language processing) to help enhance grammar, spelling, coherence and clarity.\nAs a generation of “Digital Native”, technology has been an indispensable part of our life, and it brings us both challenges and benefits. Therefore, we have to consider how to take advantage of this powerful tool. I have been working as a writing tutor for three years, and I noticed that the labor cost of reviewing students’ essays is huge. If we can make learning more efficient through the help of technology, why don’t we just do that?\nHowever, information asymmetry might be a barrier between us and advanced technology. We may not have access to the most updated educational trends and the latest technology aligning with these new trends. Therefore, I want to conduct social media analysis for education trends to provide students, parents and educators with most updated information, including the appliction of AI, NLP, etc.\n\n\nQuestions Related to This Topic\n\nWhat kinds of technology can be practically applied to education industry?\nHow are social media platforms themselves integrated into educational practices and communication?\nWhat are teachers’ and students’ views on the quality of traditional education and classroom experiences?\nWhat are the challenges and pain points discussed by educators, students, and parents in the social media?\nHow are education policies, reforms, and government decisions perceived and discussed?\nWhat are the sentiments regarding online courses and e-learning platforms?\nWhat are the most discussed educational topics on social media platforms?\nWhich subjects or fields of study are trending?\nAre there specific technological trends in educational technology (EdTech)?\nHow are emerging technologies such as AI, virtual reality, and augmented reality discussed in the context of education?\n\n\n\nSummary of Academic Publications Under this Topic\n\nFirst Paper(Kastrati et al. 2021)\n\nThe passage discusses a systematic mapping study conducted in the educational domain, focusing on sentiment analysis enabled by Natural Language Processing (NLP), machine learning, and deep learning techniques. The objective of this study was to examine students’ attitudes, opinions, and behavior towards various aspects of teaching. In this study, the researchers reviewed 92 relevant papers and analyzed them across several dimensions. These dimensions included the specific aspects of education being investigated, the most frequently referenced sources in the literature, research trends and patterns over time, the tools employed for sentiment analysis, and the common techniques used for data representation in sentiment analysis. The study identified several challenges in applying sentiment analysis to students’ feedback in education. As a result, the researchers provided recommendations and future directions to address these challenges. The aim is to inspire further research and development in the field of sentiment analysis for a better understanding of students’ feedback in an educational context.\n\nSecond Paper(Dogan, Goru Dogan, and Bozkurt 2023)\n\nThe study investigated the use of artificial intelligence (AI) in online distance education through a systematic review. It revealed a growing interest in AI technologies within education and highlighted the need for examining AI from various perspectives. The findings indicated a heavy reliance on AI technologies with potential for algorithmic-driven future scenarios. The study’s implications for future research include the need to move beyond purely technical studies and consider pedagogy, curriculum, and instructional design. It also emphasized the importance of regulating the use of human-generated data in AI applications and promoting AI ethics. Developing policies and strategies in educational institutions to ensure human-centered online learning processes was highlighted as a priority.\n\n\n\n\n\nReferences\n\nDogan, Murat Ertan, Tulay Goru Dogan, and Aras Bozkurt. 2023. “The Use of Artificial Intelligence (AI) in Online Learning and Distance Education Processes: A Systematic Review of Empirical Studies.” Applied Sciences 13 (5): 3056.\n\n\nKastrati, Zenun, Fisnik Dalipi, Ali Shariq Imran, Krenare Pireva Nuci, and Mudasir Ahmad Wani. 2021. “Sentiment Analysis of Students’ Feedback with NLP and Deep Learning: A Systematic Mapping Study.” Applied Sciences 11 (9): 3986."
  },
  {
    "objectID": "Feature selection record.html",
    "href": "Feature selection record.html",
    "title": "Feature selection for record data",
    "section": "",
    "text": "Method\nWe can use NB in a wrapper feature selection method by assessing subsets of features with the NB algorithm. The subsets that give the best performance are considered to be the most informative features.\n\n\nData Encoding\n\nFirst, let’s take an overview of our dataset.\nWe can see that all the variables are in categorical form.\n\n\n\n\noverview\n\n\n\nSince Naive Bayes requires numerical input, we need to encode the categorical variable in the numeric format.\nWe will use Python’s sklearn library and pandas library to do the encoding.\nFor binary features, we can use label encoding. For features with more than two categories, we’ll use one-hot encoding.\nAfter encoding, the dataset will appear as depicted in the following illustration.\n\n\n\n\nencoding\n\n\n\n\nFeature Selection\n\nFirst, we’ll compute the correlation matrix for the dataset to understand the relationship between variables. And below is the heatmap that helps visualize the correlation.\nFrom the heatmap, we can identify which features are most correlated with the target from the darker area.\n\n\n\n\ncorrelation matrix\n\n\n\nWe can also have a better understanding of the correlation by extracting the correlation values of the features with the target and sort them to identify the most relevant features for our model.\n\n\n\n\ncorrelation\n\n\n\nHigh Positive Correlation: Features like Financial Condition_Mid and Institution Type have the strongest positive correlation with the target and are likely good predictors.\nHigh Negative Correlation: Features like Age_6-10 and Class Duration_0 have strong negative correlations and are also likely to be informative for predicting the target.\nLow Correlation: Features closer to zero, like Network Type_3G and Device_Mobile, may not be as useful for predicting the target by themselves.\nThen we need to remove features that are highly correlated with each other, keeping only one from each group of highly correlated features to avoid multicollinearity. A threshold for identifying multicollinearity will be set. If the absolute value of the correlation coefficient between two features is higher than this threshold, those features are considered multicollinear.\nResult: we will remove three features, which are ‘Education Level_School’, ‘Age_21-25’ and ‘Network Type_3G’."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "simple_quarto_website",
    "section": "",
    "text": "switzerland is a a good place for travel. Below are some recommended places to visit.\n\n\nName\nBrienz\nGrindelwald\nEiger\n\nLocation\nBernese Oberland region\nInterlaken-Oberhasli\nBernese Alps"
  },
  {
    "objectID": "index.html#section-1",
    "href": "index.html#section-1",
    "title": "simple_quarto_website",
    "section": "",
    "text": "switzerland is a a good place for travel. Below are some recommended places to visit.\n\n\nName\nBrienz\nGrindelwald\nEiger\n\nLocation\nBernese Oberland region\nInterlaken-Oberhasli\nBernese Alps"
  },
  {
    "objectID": "index.html#section-2",
    "href": "index.html#section-2",
    "title": "simple_quarto_website",
    "section": "2 Section 2",
    "text": "2 Section 2\nBulleted List for Swiss chocolate brands:\n\nToblerone\nLäderach\nLindt\n\n\n\n\nLake Name\nLocation\n\n\n\n\nLake Brienz\nBern\n\n\nLake Lauerz\nSchwyz\n\n\n\nMath Equation1: \\(y = ax + b\\)\n\\[\nE=mc^2\n\\]\n\nthis is a quote"
  },
  {
    "objectID": "index.html#section-3",
    "href": "index.html#section-3",
    "title": "simple_quarto_website",
    "section": "3 Section 3",
    "text": "3 Section 3\n\n\n\nSwiss image1\n\n\n\n\n\nSwiss image2\n\n\n-\n\n\n\n\nflowchart LR\n  A[food] --&gt; B(sweets)\n  B --&gt; C{what}\n  C --&gt; D[bubble tea]\n  C --&gt; E[cupcake]\n\n\n\n\n\nSchool anxiety is one of the main problems faced by teachers and school counselors. This problem attracts special attention for teachers - practitioners, as it is a clear sign of school maladaptation in children, which adversely affects all spheres of their life: not only the studies but also communication, both inside and outside the school, health and general level of mental development and psychological well-being. (Kostyunina and Drozdikova-Zaripova 2016)"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "simple_quarto_website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere is the footnote.↩︎"
  },
  {
    "objectID": "Data Gathering.html",
    "href": "Data Gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "** First Part **\nNumber of Virtual Schools in Year 2015 and Year 2020\n\nCollect Method: Urban Institute’s Education Data API Urban Institute\n\n\n\n\nDatasets Name Raw Data Access\ndescription\nformat\n\n\n\n\nvirtual_school_2015\n709 rows, 52 columns\n.csv\n\n\nvirtual_school_2020\n818 rows, 52 columns\n.csv\n\n\n\nRaw Data Overview\n\nThese two datasets are basically same in terms of structure and variables, so I just use one of them as an example\n\n\n\n\nvirtual_school_2015\n\n\n\n\n\nvirtual_school_2015\n\n\n\n\n** Second Part **\nDistance Education in Title IV Institutions\n\nCollect Method: Download from a website called STEM Education Data\n\n\n\n\nDatasets Name Raw Data Access\ndescription\nformat\n\n\n\n\ndistance_education\n31 rows, 5 columns\n.xls\n\n\n\nRaw Data Overview\n\n\n\ndistance_education\n\n\n\n\n** Third Part **\nDistance education in Title IV institutions\n\nCollect Method: Download from an organization called OECD\n\n\n\n\nDatasets Name Raw Data Access\ndescription\nformat\n\n\n\n\nAI_pub\n264 rows, 3 columns\n.csv\n\n\n\nRaw Data Overview\n\n\n\nAI_pub\n\n\n\n\n** Fourth Part **\nEducation Trend Text Data\n\nCollect Method: NewsAPI\n\n\n\n\nDatasets Name Raw Data Access\ndescription\nformat\n\n\n\n\nAI_pub\n100 documents\n.json\n\n\n\nRaw Data Overview\n\n\n\neducation_trend"
  },
  {
    "objectID": "Final results text.html",
    "href": "Final results text.html",
    "title": "Final results(text)",
    "section": "",
    "text": "Results from the Naive Bayes Model\n\nUsing optimal feature set from the previous section and fitting a final “optimal” NB model, we get the following results.\n\n\n\nresult\n\n\n\n\n\nEvaluation and Discussion\n\nThe overall accuracy of the model is 45%. This means that out of all the predictions made by the model, 45% of them were correct. While accuracy is a useful metric, it doesn’t tell the whole story, especially in cases where class distribution is imbalanced.\nThe “Education” class has perfect precision but very low recall, suggesting that when the classifier predicts an article as “Education”, it is always correct, but it fails to identify most of the actual “Education” articles. The “Other” class has the highest F1-score, showing a better balance between precision and recall, but the precision is still under 50%. The classes “Entertainment”, “Health”, and “Politics” have zero precision and recall, which might be due to very few samples being present in the test set, making it difficult for the model to learn to predict these classes correctly. The “Technology” class has low precision and recall, indicating a struggle to correctly classify articles of this type.\n\n\n\nVisualizations of the results\n\nThe confusion matrix below shows the true labels vs. the predicted labels for each class. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while the off-diagonal elements are those that are mislabeled by the classifier.\n\n\n\n\nconfusion matrix\n\n\n\nThis bar chart shows the precision, recall, and F1-score for each class. Precision is the ability of the classifier not to label a negative sample as positive, recall is the ability to find all positive samples, and the F1-score is a harmonic mean of precision and recall. This visualization makes it easier to compare these metrics across different classes.\n\n\n\n\nPrecision, Recall, and F1-Score\n\n\n\nThis bar graph illustrates the count of actual occurrences for each category within the testing dataset. It serves a crucial role in comprehending the distribution of these categories and pinpointing any potential class imbalances. Classes that have a limited number of instances can pose challenges for the classifier in making accurate predictions, potentially impacting the overall performance of the model.\n\n\n\n\nSupport for each class"
  },
  {
    "objectID": "Introduction to Naive Bayes.html",
    "href": "Introduction to Naive Bayes.html",
    "title": "Introduction to Naive Bayes",
    "section": "",
    "text": "Overview\nNaive Bayes is a classification algorithm based on Bayes’ theorem, which describes the probability of an event based on prior knowledge of conditions related to the event. It is called “naive” because it makes the simplifying assumption that the features used to make the classification decision are conditionally independent given the class label.\nThe Naive Bayes classifier is based on Bayes’ theorem, which is expressed as:\n\n\n\nformula\n\n\nSource: Chatgpt\n\n\nProbabilistic Nature of Naive Bayes\nNaive Bayes is a probabilistic classifier, meaning that for a given input, it predicts the probability of each possible class label and selects the label with the highest probability. This approach allows it to quantify uncertainty and provide more information about the predictions it makes, which can be crucial in decision-making processes.\n\n\nObjectives and Plans through Naive Bayes classification\nPredicting Adaptability Level: Predict the adaptability level of students to online education based on various features present in the dataset.\nTarget Variable: Adaptability Level  Features: Other variables in the dataset which could include demographic information, access to resources, previous online education experience, etc.\n\n\nDifferent Variants of Naive Bayes\n\nGaussian Naive Bayes assumes that the features follow a Gaussian or normal distribution. It is well-suited for continuous data, making it a popular choice in fields like medicine for predicting outcomes based on various continuous test results. For instance, it could be used to predict whether a tumor is malignant or benign based on features like size, shape, and texture.\nMultinomial Naive Bayes operates on discrete data representing counts or frequencies of events. This variant excels in text classification tasks, such as spam detection or sentiment analysis, where features might represent the frequency of words in a document. A typical application could be classifying news articles into different categories based on the occurrence of specific terms.\nBernoulli Naive Bayes is designed to work with binary or boolean features, indicating the presence or absence of a feature. It is also commonly used in text classification, where features could represent the occurrence of words in a document. For example, it might be used to detect spam emails based on whether certain trigger words are present or absent."
  },
  {
    "objectID": "About Me.html",
    "href": "About Me.html",
    "title": "About Me",
    "section": "",
    "text": "Hello Message\nHello!My name is Lan Cao (GU NetID: lc1311).\n\n\nBackground\nLan Cao comes from China, and she is now studying at the Data Science & Analytics Program at Georgetown University. Lan received her bachelor’s degree from Rutgers University (The State University of New Jersey) in May 2023. During undergraduate studies, Lan majored in Economics and minor in Statistics and Data Science. She worked as a writing tutor at Rutgers Writing Centers for two years and she also served as a writing tutor in the Rutgers EOF program in summer 2022. After graduation from Rutgers, Lan is now working as a part-time writing tutor in a company focusing on education services. Except for tutoring, Lan also has a great passion for cooking, and she is good at making traditional Chinese dishes.\n\n\n\n\n\n\nLan Cao\n\n\n\n\n\n\n\nEducation\n\n2023-2025: Georgetown Univeristy, D.C.\n2020-2023: Rutgers Univeristy, New Brunswick, NJ\n2018-2020: Jilin Univeristy, Changchun, China\n\n\n\nAcademic Interests\n\nData Visualization\nHealth Data Analysis\nNLP"
  },
  {
    "objectID": "Code.html",
    "href": "Code.html",
    "title": "Code",
    "section": "",
    "text": "GitHub"
  },
  {
    "objectID": "Data Exploration.html",
    "href": "Data Exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Exploratory Data Analysis (EDA)\nI have several datasets that will be used for this project, and here I will do EDA mainly for the “students_adaptability_level_online_education” dataset.\n\n\nData Understanding:\n\nData Source: Students’ Adaptability Level Prediction in Online Education using Machine Learning Approaches (DOI: 10.1109/ICCCNT51525.2021.9579741) You can find the raw data here\nData Information: There are 14 features in total and all of them are ‘object’ type.\n\n\n\n\ndata info\n\n\n\n\nDescriptive Statistics:\n\nNotes: Most of the data in this dataset is in categorical form, so we will not try to compute descriptive statistics like mean or median. Instead, we will provide frequency distributions to get a rough sense of how these data are distributed.\nComments: Here we can see that some of the variables are binary variables and most of them contains less than 5 values.\n\n\n\n\nfrequency\n\n\n\n\nData Visualization:\n\nComments: First we do univariate analysis for our dataset. These visualizations provide a good overview of the distribution of different variables in the dataset.\n\nKey Observations:\n\nThere are more boys than girls in the dataset. Most students are in the 21-25 age range, followed by 16-20. There are fewer students in the other age ranges.\nA majority of students are from non-government institutions and most students are not IT students.\nMost students have a proper place to participate in online classes. More students use mobile data compared to Wi-Fi.\nMost students are from families with a mid-level financial condition, followed by poor, and then rich.\n\n\n\n\nUnivariate\n\n\n\n\nCorrelation Analysis:\n\nComments: correlation analysis typically applies to numerical data, but all the variables in our dataset are categorical. For categorical variables, we can use other methods like Heatmaps to show associations or dependencies between variables. The plots below can give us some insights into our variables.\n\nKey Findings:\n\nBoth genders show a similar distribution across different adaptivity levels, with the majority in the moderate adaptivity level.\nUniversity students are more likely to have a high adaptivity level compared to school and college students.\nStudents from mid financial condition families are spread across all adaptivity levels, with the majority in the moderate level. Students from rich families are mostly in the moderate and high adaptivity levels, with very few in the low category.\nStudents using computers tend to have higher adaptivity levels compared to those using mobiles or tablets.\n\n\n\n\nBivariate\n\n\n\n\n\nheatmap\n\n\n\n\nHypothesis Generation:\nNotes: Part of my project is to examine what kind of factors can influence student’s online learning. Therefore, we can generate some hypothesis regarding this topic.\nHypothesis:\n\nGender and Adaptivity Level:\n\nNull Hypothesis: There is no significant difference in adaptivity levels between male and female students.\nAlternative Hypothesis: There is a significant difference in adaptivity levels between male and female students.\n\nFinancial Condition and Adaptivity Level:\n\nNull Hypothesis: The financial condition of a student’s family does not affect their adaptivity level to online education.\nAlternative Hypothesis: The financial condition of a student’s family affects their adaptivity level to online education.\n\n\n\n\nData Grouping and Segmentation:\n\nNotes: Here we group the data by education level (University, College, School) to analyze how adaptivity levels and other factors vary across different education levels.\nFindings: The gender distribution is fairly balanced across all education levels; Students from mid financial conditions are prevalent across all education levels; There is a higher proportion of rich students in the university level compared to school and college.\n\n\n\n\neducation-level\n\n\n\n\nIdentifying Outliers:\n\nNotes: Outlier detection is typically applied to numerical data, where statistical methods can be used to identify values that deviate significantly from the rest of the data. However, the dataset does not contain any numerical columns, so our approach to outlier detection will need to be adjusted. We can still investigate the distribution of values in the categorical columns to identify any unusual or unexpected patterns that might indicate data quality issues.\nFindings: Based on the unique value counts for each categorical column, there don’t appear to be any blatant anomalies or irregularities in the dataset. The values in each column seem to be consistent and within expected ranges. However, there are a few points worth noting: for example, the “0” value in the “Class Duration” column could potentially be an anomaly or a data entry error, as it’s not clear what “0” hours of class duration would mean in the context of online education. It might be worth investigating these entries further to understand if they are valid or if they represent a data quality issue.\n\n\n\n\nReport and discussion of methods and findings:\n\nThe dataset contains 1205 entries with 14 categorical variables.There are no missing values, and all categorical variables contain expected and consistent values\nWhen delving into individual variables, we observe a gender distribution skewing slightly towards boys, and a prominent presence of students in the 11-25 age range, with the majority being at the school or university level. A significant portion of these students come from non-government institutions and reside in urban areas. Despite the limited number of IT students, the dataset provides a broad overview of the student demographic.\nIn terms of technology access and internet connectivity, a larger number of students have mobile data connections, primarily on 4G networks, and use mobile devices for online education. This is coupled with a general experience of low load-shedding across the board. Focusing on their online education experience, most students have 1-3 hours of online classes, but there is a noticeable lack of engagement with Learning Management Systems.\nWhen examining the adaptability levels in relation to other variables, certain patterns emerge. Higher adaptability is often linked with being an IT student, having stable internet connectivity (preferably wifi), using computers for online education, engaging with longer online classes, and having access to Learning Management Systems. Financial stability also plays a crucial role, with students from more affluent backgrounds exhibiting higher adaptability levels. Conversely, lower adaptability levels are more common among younger students, those from less affluent backgrounds, those experiencing frequent load-shedding, and those relying on mobile devices for online education.\nThese insights underscore the importance of technological access and financial stability in fostering a conducive environment for online education. They highlight potential areas for intervention, such as improving internet connectivity, promoting the use of Learning Management Systems, and offering additional support to students from less privileged backgrounds to enhance their adaptability and overall experience in an online learning setting.\n\n\n\nTools and Software:\nPython: pandas;matplotlib;seaborn"
  },
  {
    "objectID": "slides/slides.html#brienz",
    "href": "slides/slides.html#brienz",
    "title": "Slide",
    "section": "Brienz",
    "text": "Brienz\n\nSwiss image1"
  },
  {
    "objectID": "slides/slides.html#grindelwald",
    "href": "slides/slides.html#grindelwald",
    "title": "Slide",
    "section": "Grindelwald",
    "text": "Grindelwald\n\nSwiss image1"
  },
  {
    "objectID": "slides/slides.html#eiger",
    "href": "slides/slides.html#eiger",
    "title": "Slide",
    "section": "Eiger",
    "text": "Eiger\nSchool anxiety is one of the main problems faced by teachers and school counselors. This problem attracts special attention for teachers - practitioners, as it is a clear sign of school maladaptation in children, which adversely affects all spheres of their life: not only the studies but also communication, both inside and outside the school, health and general level of mental development and psychological well-being. (Kostyunina and Drozdikova-Zaripova 2016)\n\n\n\n\n\n\n\nKostyunina, Nadezhda Yu, and Albina R Drozdikova-Zaripova. 2016. “Adolescents’ School Anxiety Correction by Means of Mandala Art Therapy.” International Journal of Environmental and Science Education 11 (6): 1105–16."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#part1",
    "href": "about.html#part1",
    "title": "About",
    "section": "part1",
    "text": "part1\nI like chocolate."
  },
  {
    "objectID": "about.html#part2",
    "href": "about.html#part2",
    "title": "About",
    "section": "part2",
    "text": "part2\nI like bubble tea."
  },
  {
    "objectID": "about.html#part3",
    "href": "about.html#part3",
    "title": "About",
    "section": "part3",
    "text": "part3\nI like coffee.\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [5, 7, 5, 8, 3]\n\na = plt.plot(x, y)\n\nprint(a)\n\n\n\n\n[&lt;matplotlib.lines.Line2D object at 0x7fdb3940ac50&gt;]"
  },
  {
    "objectID": "Data Cleaning.html",
    "href": "Data Cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This is Just Initial Data Cleaning\n\nI will only focus on the first part and the fourth part because they are pretty messy\nThe second and third part will be cleaned later because they are relatively easy to clean\n\n\n\nFirst Part\n\nClean “virtual_school_2015.csv”\nYou can see and get the row data here\nClean Notes: since “virtual_school_2015.csv” and “virtual_school_2020” are very similar in terms of structure and variables, so here we will just how the cleaning process of “virtual_school_2015.csv” as an demonstration\nClean Plan: “virtual_school_2015.csv” is a large dataset that contains 52 variables and I plan to use some of them, so I will subset the dataset first. And then I will change some quantative variables to be qualitative variables because some data was transformed into numeric values and lose their original meanings. Therefore, I need to bring their original meaning back for future use.\n\nBefore(this is raw data)\n\nWe can see there are a lot of variables there, so I plan to make a subset\n\n\n\n\nvirtual_school_2015\n\n\nDuring Data Cleaning\n\nYou can get the code here to see how the data was gathered and cleaned\nI use tidyverse package in R to do the cleaning\nFirst choose some variables that I want to use to make a subset\n\n\n\n\nsubset of virtual_school_2015\n\n\n\nThen we will filter the missing values. I don’t plan to replace the missing values because the enrollment of students is unique to every school, therefore making it meaningless to replace the missing enrollment value with any other kind of numbers. Before filtering and subsetting, the dataset has 709 rows, 52 columns. After filtering and subsetting, it has 660 rows, 9 columns. We lost around 7% data\nThe we replace the numeric values of “school_level”, “school_type”, “charter”, “magnet” to be qualitative values\n\nAfter(this is a cleaned version)\n\nYou can get the cleaned data here\n\n\n\n\ncleaned_subset of virtual_school_2015\n\n\n\n\nSecond Part\n\nIt will be cleaned later\n\n\n\nThird Part\n\nIt will be cleaned later\n\n\n\nFourth Part\n\nClean text data “education_trend.json”\nYou can get the raw data here\nWe need to vectorize the data first\n\nBefore(this is raw data)\n\n\n\neudcation_trend\n\n\nAfter(this is a cleaned version)\n\nYou can find the cleaned text data here\n\n\n\n\ntransformed_dataframe"
  },
  {
    "objectID": "Feature selection text.html",
    "href": "Feature selection text.html",
    "title": "Feature selection for text data",
    "section": "",
    "text": "Method\n\nData preprocessing involves the conversion of text data into a numerical format, which is essential for training machine learning models like Naive Bayes. This transformation can be achieved using methods like TF-IDF (Term Frequency-Inverse Document Frequency) or Count Vectorization, which help represent the text as numerical features that can be utilized effectively by the Naive Bayes algorithm.\nWe will vectorize the titles using Count Vectorization. When we convert text data into a numerical format using Count Vectorization, each unique word or token in the text becomes a feature. The value of the feature is the count of how many times that word appears in the document. In our case, the document specifically refers to titles of the news paper. So if our dataset has 10,000 unique words across all the titles, we have 10,000 features.\nFeature selection is a technique used to enhance a model’s performance and mitigate overfitting by picking out the most pertinent features or words from a larger pool. This process involves reducing the total number of unique words to a smaller, more relevant subset that can effectively predict the article’s topic or class labels.\nWe will apply feature selection using the chi-squared test to reduce the number of features to the most informative ones. In text data, there can be a vast number of features (every unique word can be a feature), many of which may not be useful for making predictions. The chi-squared test is a way to handle this high dimensionality by filtering out the noise and keeping only those features that are statistically significant.\nBelow is part of the selected features (Top 20)\n\n\n\n\nfeature selection"
  }
]
[
  {
    "objectID": "Data Preparation.html",
    "href": "Data Preparation.html",
    "title": "Data Preparation",
    "section": "",
    "text": "Why splitting our dataset?\nProperly splitting the dataset is fundamental to building a robust and generalizable machine learning model. The training set allows the model to learn, the validation set helps in tuning and selecting the best model, and the testing set provides an unbiased evaluation of the final model’s performance.\n\n\nHow to split our dataset?\nStratified Split: Given that the target variable is “Adaptivity Level” (which is categorical), we might want to ensure that the distribution of classes in the target variable is preserved in both the training and testing sets. This is especially important if the classes are imbalanced. A stratified split would help in maintaining the class distribution."
  },
  {
    "objectID": "Final results record.html",
    "href": "Final results record.html",
    "title": "Final results(record)",
    "section": "",
    "text": "Results from the Navie Bayes Model\n\nUsing optimal feature set from the previous section and fitting a final “optimal” NB model, we get the following results.\n\n\n\n\nresult\n\n\n\n\nEvaluation\n\nAccuracy: the accuracy of the model is 63.81%, which means that the classifier correctly predicted the adaptivity level for approximately 64 out of every 100 students.\nPrecision: the precision for the classes 0, 1, and 2 are 0.49, 0.66, and 0.65, respectively. This means that when the model predicts a student’s adaptivity level as class 0, it is correct about 49% of the time. For class 1, it is correct 66% of the time, and for class 2, 65% of the time.\nRecall: the recall for the classes 0, 1, and 2 are 0.46, 0.71, and 0.61, respectively. This implies that the model identifies 46% of all actual class 0 adaptivity levels correctly, 71% of class 1, and 61% of class 2.\nF1-scores: the F1-scores for the classes 0, 1, and 2 are 0.47, 0.69, and 0.63, respectively. The F1-score balances the trade-off between precision and recall. For class 1, the balance is better (F1-score 0.69) compared to class 0 (F1-score 0.47) and class 2 (F1-score 0.63), indicating a more reliable performance for class 1 predictions.\nSupport: There were 39 instances of class 0, 154 of class 1, and 169 of class 2 in the test set. This could suggest why the classifier’s performance for class 0 might not be as high as for the other classes, as there are fewer examples to learn from.\n\n\n\nDiscussion\n\nThe data is split such that a certain percentage ( 70% for training and 30% for testing). The Naive Bayes classifier was trained on the training dataset. During this phase, the model learns the relationship between the features of the input data and the target variable. It does this by analyzing the distribution of the features and computing the conditional probability of each class given an input.\nWhen we divide our data into training and testing sets, it’s important to note that the model doesn’t have any prior knowledge of the testing set. The testing set comprises both the features (independent variables) and the actual labels (the ground truth we aim to predict). To evaluate the model’s performance, we utilize a trained Naive Bayes model and apply it to predict the labels for the testing data. This involves feeding the testing set’s features into the model’s prediction function. The model then uses the probability information it acquired during training to determine the most probable label for each instance within the testing set. The predictions generated by the model for the testing set are subsequently compared against the actual labels of the testing set. This comparison serves as the test of the model’s predictive accuracy.\n\n\n\nVisualizations of the results\n\nThe confusion matrix below shows the true labels vs. the predicted labels for each class. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while the off-diagonal elements are those that are mislabeled by the classifier.\n\n\n\n\nconfusion matrix\n\n\n\nThe training score (blue line) is the accuracy of the model on the training set. A high score indicates that the model performs well on the training data. The cross-validation score (red line) is the accuracy of the model on the validation set. A high score indicates that the model generalizes well to unseen data. The shaded areas represent the variability (standard deviation) of the scores around the mean.\nAs the number of training samples increases, the training score decreases, which is typical because models tend to perform better when they are trained on fewer samples (they can memorize the training data). The cross-validation score generally increases with more data, which indicates that the model is generalizing better with more data. The convergence of the training and validation scores suggests that adding more training samples may not lead to significant improvements in the model’s performance. This can indicate that the model has reached its performance limit given the current features and model complexity.\n\n\n\n\nlearning curve\n\n\n\nA validation curve is a tool used to improve the performance of a machine learning model by tuning hyperparameters. It shows the relationship between a single hyperparameter value and the model’s performance, which is measured by some scoring metric (e.g., accuracy). By observing how the training score and the cross-validation score change with different values of a hyperparameter, we can choose the best value that will hopefully generalize well to unseen data.\nFor the Naive Bayes classifier we’ve been using, a key hyperparameter to vary could be the var_smoothing parameter of the GaussianNB classifier, which is used to specify the portion of the largest variance of all features that is added to variances for calculation stability.\nFrom the plot, we can observe how the accuracy of the model changes with different values of var_smoothing. Ideally, we’re looking for a value where the cross-validation score is high, and the gap between the training and cross-validation scores is relatively small, indicating that the model generalizes well without overfitting.\n\n\n\n\nValidation Curve\n\n\n\nNote: documentation process is facilitated by Chatgpt"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Social Media Analysis for Education Trends\nAuthor: Lan Cao\n\n\nBackground\nTechnology has reshaped different aspects of our life, and one of them is education. Covid-19 made online courses become an indispensable part of our daily study. Online learning platforms like Coursera and Udemy are proving people with tons of opportunities of absorbing knowledge from many industries. Artificial Intelligence has also been applied to helping students improving their reading, writing and speaking. For example, Grammarly makes use of NLP(natural language processing) to help enhance grammar, spelling, coherence and clarity.\nAs a generation of “Digital Native”, technology has been an indispensable part of our life, and it brings us both challenges and benefits. Therefore, we have to consider how to take advantage of this powerful tool. I have been working as a writing tutor for three years, and I noticed that the labor cost of reviewing students’ essays is huge. If we can make learning more efficient through the help of technology, why don’t we just do that?\nHowever, information asymmetry might be a barrier between us and advanced technology. We may not have access to the most updated educational trends and the latest technology aligning with these new trends. Therefore, I want to conduct social media analysis for education trends to provide students, parents and educators with most updated information, including the appliction of AI, NLP, etc.\n\n\nQuestions Related to This Topic\n\nWhat kinds of technology can be practically applied to education industry?\nHow are social media platforms themselves integrated into educational practices and communication?\nWhat are teachers’ and students’ views on the quality of traditional education and classroom experiences?\nWhat are the challenges and pain points discussed by educators, students, and parents in the social media?\nHow are education policies, reforms, and government decisions perceived and discussed?\nWhat are the sentiments regarding online courses and e-learning platforms?\nWhat are the most discussed educational topics on social media platforms?\nWhich subjects or fields of study are trending?\nAre there specific technological trends in educational technology (EdTech)?\nHow are emerging technologies such as AI, virtual reality, and augmented reality discussed in the context of education?\n\n\n\nSummary of Academic Publications Under this Topic\n\nFirst Paper(Kastrati et al. 2021)\n\nThe passage discusses a systematic mapping study conducted in the educational domain, focusing on sentiment analysis enabled by Natural Language Processing (NLP), machine learning, and deep learning techniques. The objective of this study was to examine students’ attitudes, opinions, and behavior towards various aspects of teaching. In this study, the researchers reviewed 92 relevant papers and analyzed them across several dimensions. These dimensions included the specific aspects of education being investigated, the most frequently referenced sources in the literature, research trends and patterns over time, the tools employed for sentiment analysis, and the common techniques used for data representation in sentiment analysis. The study identified several challenges in applying sentiment analysis to students’ feedback in education. As a result, the researchers provided recommendations and future directions to address these challenges. The aim is to inspire further research and development in the field of sentiment analysis for a better understanding of students’ feedback in an educational context.\n\nSecond Paper(Dogan, Goru Dogan, and Bozkurt 2023)\n\nThe study investigated the use of artificial intelligence (AI) in online distance education through a systematic review. It revealed a growing interest in AI technologies within education and highlighted the need for examining AI from various perspectives. The findings indicated a heavy reliance on AI technologies with potential for algorithmic-driven future scenarios. The study’s implications for future research include the need to move beyond purely technical studies and consider pedagogy, curriculum, and instructional design. It also emphasized the importance of regulating the use of human-generated data in AI applications and promoting AI ethics. Developing policies and strategies in educational institutions to ensure human-centered online learning processes was highlighted as a priority.\n\n\n\n\n\nReferences\n\nDogan, Murat Ertan, Tulay Goru Dogan, and Aras Bozkurt. 2023. “The Use of Artificial Intelligence (AI) in Online Learning and Distance Education Processes: A Systematic Review of Empirical Studies.” Applied Sciences 13 (5): 3056.\n\n\nKastrati, Zenun, Fisnik Dalipi, Ali Shariq Imran, Krenare Pireva Nuci, and Mudasir Ahmad Wani. 2021. “Sentiment Analysis of Students’ Feedback with NLP and Deep Learning: A Systematic Mapping Study.” Applied Sciences 11 (9): 3986."
  },
  {
    "objectID": "Feature selection record.html",
    "href": "Feature selection record.html",
    "title": "Feature selection for record data",
    "section": "",
    "text": "Method\nWe can use NB in a wrapper feature selection method by assessing subsets of features with the NB algorithm. The subsets that give the best performance are considered to be the most informative features.\n\n\nData Encoding\n\nFirst, let’s take an overview of our dataset.\nWe can see that all the variables are in categorical form.\n\n\n\n\noverview\n\n\n\nSince Naive Bayes requires numerical input, we need to encode the categorical variable in the numeric format.\nWe will use Python’s sklearn library and pandas library to do the encoding.\nFor binary features, we can use label encoding. For features with more than two categories, we’ll use one-hot encoding.\nAfter encoding, the dataset will appear as depicted in the following illustration.\n\n\n\n\nencoding\n\n\n\n\nFeature Selection\n\nFirst, we’ll compute the correlation matrix for the dataset to understand the relationship between variables. And below is the heatmap that helps visualize the correlation.\nFrom the heatmap, we can identify which features are most correlated with the target from the darker area.\n\n\n\n\ncorrelation matrix\n\n\n\nWe can also have a better understanding of the correlation by extracting the correlation values of the features with the target and sort them to identify the most relevant features for our model.\n\n\n\n\ncorrelation\n\n\n\nHigh Positive Correlation: Features like Financial Condition_Mid and Institution Type have the strongest positive correlation with the target and are likely good predictors.\nHigh Negative Correlation: Features like Age_6-10 and Class Duration_0 have strong negative correlations and are also likely to be informative for predicting the target.\nLow Correlation: Features closer to zero, like Network Type_3G and Device_Mobile, may not be as useful for predicting the target by themselves.\nThen we need to remove features that are highly correlated with each other, keeping only one from each group of highly correlated features to avoid multicollinearity. A threshold for identifying multicollinearity will be set. If the absolute value of the correlation coefficient between two features is higher than this threshold, those features are considered multicollinear.\nResult: we will remove three features, which are ‘Education Level_School’, ‘Age_21-25’ and ‘Network Type_3G’."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "simple_quarto_website",
    "section": "",
    "text": "switzerland is a a good place for travel. Below are some recommended places to visit.\n\n\nName\nBrienz\nGrindelwald\nEiger\n\nLocation\nBernese Oberland region\nInterlaken-Oberhasli\nBernese Alps"
  },
  {
    "objectID": "index.html#section-1",
    "href": "index.html#section-1",
    "title": "simple_quarto_website",
    "section": "",
    "text": "switzerland is a a good place for travel. Below are some recommended places to visit.\n\n\nName\nBrienz\nGrindelwald\nEiger\n\nLocation\nBernese Oberland region\nInterlaken-Oberhasli\nBernese Alps"
  },
  {
    "objectID": "index.html#section-2",
    "href": "index.html#section-2",
    "title": "simple_quarto_website",
    "section": "2 Section 2",
    "text": "2 Section 2\nBulleted List for Swiss chocolate brands:\n\nToblerone\nLäderach\nLindt\n\n\n\n\nLake Name\nLocation\n\n\n\n\nLake Brienz\nBern\n\n\nLake Lauerz\nSchwyz\n\n\n\nMath Equation1: \\(y = ax + b\\)\n\\[\nE=mc^2\n\\]\n\nthis is a quote"
  },
  {
    "objectID": "index.html#section-3",
    "href": "index.html#section-3",
    "title": "simple_quarto_website",
    "section": "3 Section 3",
    "text": "3 Section 3\n\n\n\nSwiss image1\n\n\n\n\n\nSwiss image2\n\n\n-\n\n\n\n\nflowchart LR\n  A[food] --&gt; B(sweets)\n  B --&gt; C{what}\n  C --&gt; D[bubble tea]\n  C --&gt; E[cupcake]\n\n\n\n\n\nSchool anxiety is one of the main problems faced by teachers and school counselors. This problem attracts special attention for teachers - practitioners, as it is a clear sign of school maladaptation in children, which adversely affects all spheres of their life: not only the studies but also communication, both inside and outside the school, health and general level of mental development and psychological well-being. (Kostyunina and Drozdikova-Zaripova 2016)"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "simple_quarto_website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere is the footnote.↩︎"
  },
  {
    "objectID": "Data Gathering.html",
    "href": "Data Gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "** First Part **\nNumber of Virtual Schools in Year 2015 and Year 2020\n\nCollect Method: Urban Institute’s Education Data API Urban Institute\n\n\n\n\nDatasets Name Raw Data Access\ndescription\nformat\n\n\n\n\nvirtual_school_2015\n709 rows, 52 columns\n.csv\n\n\nvirtual_school_2020\n818 rows, 52 columns\n.csv\n\n\n\nRaw Data Overview\n\nThese two datasets are basically same in terms of structure and variables, so I just use one of them as an example\n\n\n\n\nvirtual_school_2015\n\n\n\n\n\nvirtual_school_2015\n\n\n\n\n** Second Part **\nDistance Education in Title IV Institutions\n\nCollect Method: Download from a website called STEM Education Data\n\n\n\n\nDatasets Name Raw Data Access\ndescription\nformat\n\n\n\n\ndistance_education\n31 rows, 5 columns\n.xls\n\n\n\nRaw Data Overview\n\n\n\ndistance_education\n\n\n\n\n** Third Part **\nDistance education in Title IV institutions\n\nCollect Method: Download from an organization called OECD\n\n\n\n\nDatasets Name Raw Data Access\ndescription\nformat\n\n\n\n\nAI_pub\n264 rows, 3 columns\n.csv\n\n\n\nRaw Data Overview\n\n\n\nAI_pub\n\n\n\n\n** Fourth Part **\nEducation Trend Text Data\n\nCollect Method: NewsAPI\n\n\n\n\nDatasets Name Raw Data Access\ndescription\nformat\n\n\n\n\nAI_pub\n100 documents\n.json\n\n\n\nRaw Data Overview\n\n\n\neducation_trend"
  },
  {
    "objectID": "Final results text.html",
    "href": "Final results text.html",
    "title": "Final results(text)",
    "section": "",
    "text": "Results from the Naive Bayes Model\n\nUsing optimal feature set from the previous section and fitting a final “optimal” NB model, we get the following results.\n\n\n\nresult\n\n\n\n\n\nEvaluation and Discussion\n\nThe overall accuracy of the model is 45%. This means that out of all the predictions made by the model, 45% of them were correct. While accuracy is a useful metric, it doesn’t tell the whole story, especially in cases where class distribution is imbalanced.\nThe “Education” class has perfect precision but very low recall, suggesting that when the classifier predicts an article as “Education”, it is always correct, but it fails to identify most of the actual “Education” articles. The “Other” class has the highest F1-score, showing a better balance between precision and recall, but the precision is still under 50%. The classes “Entertainment”, “Health”, and “Politics” have zero precision and recall, which might be due to very few samples being present in the test set, making it difficult for the model to learn to predict these classes correctly. The “Technology” class has low precision and recall, indicating a struggle to correctly classify articles of this type.\n\n\n\nVisualizations of the results\n\nThe confusion matrix below shows the true labels vs. the predicted labels for each class. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while the off-diagonal elements are those that are mislabeled by the classifier.\n\n\n\n\nconfusion matrix\n\n\n\nThis bar chart shows the precision, recall, and F1-score for each class. Precision is the ability of the classifier not to label a negative sample as positive, recall is the ability to find all positive samples, and the F1-score is a harmonic mean of precision and recall. This visualization makes it easier to compare these metrics across different classes.\n\n\n\n\nPrecision, Recall, and F1-Score\n\n\n\nThis bar graph illustrates the count of actual occurrences for each category within the testing dataset. It serves a crucial role in comprehending the distribution of these categories and pinpointing any potential class imbalances. Classes that have a limited number of instances can pose challenges for the classifier in making accurate predictions, potentially impacting the overall performance of the model.\n\n\n\n\nSupport for each class"
  },
  {
    "objectID": "Introduction to Naive Bayes.html",
    "href": "Introduction to Naive Bayes.html",
    "title": "Introduction to Naive Bayes",
    "section": "",
    "text": "Overview\nNaive Bayes is a classification algorithm based on Bayes’ theorem, which describes the probability of an event based on prior knowledge of conditions related to the event. It is called “naive” because it makes the simplifying assumption that the features used to make the classification decision are conditionally independent given the class label.\nThe Naive Bayes classifier is based on Bayes’ theorem, which is expressed as:\n\n\n\nformula\n\n\nSource: Chatgpt\n\n\nProbabilistic Nature of Naive Bayes\nNaive Bayes is a probabilistic classifier, meaning that for a given input, it predicts the probability of each possible class label and selects the label with the highest probability. This approach allows it to quantify uncertainty and provide more information about the predictions it makes, which can be crucial in decision-making processes.\n\n\nObjectives and Plans through Naive Bayes classification\nPredicting Adaptability Level: Predict the adaptability level of students to online education based on various features present in the dataset.\nTarget Variable: Adaptability Level  Features: Other variables in the dataset which could include demographic information, access to resources, previous online education experience, etc.\n\n\nDifferent Variants of Naive Bayes\n\nGaussian Naive Bayes assumes that the features follow a Gaussian or normal distribution. It is well-suited for continuous data, making it a popular choice in fields like medicine for predicting outcomes based on various continuous test results. For instance, it could be used to predict whether a tumor is malignant or benign based on features like size, shape, and texture.\nMultinomial Naive Bayes operates on discrete data representing counts or frequencies of events. This variant excels in text classification tasks, such as spam detection or sentiment analysis, where features might represent the frequency of words in a document. A typical application could be classifying news articles into different categories based on the occurrence of specific terms.\nBernoulli Naive Bayes is designed to work with binary or boolean features, indicating the presence or absence of a feature. It is also commonly used in text classification, where features could represent the occurrence of words in a document. For example, it might be used to detect spam emails based on whether certain trigger words are present or absent."
  },
  {
    "objectID": "About Me.html",
    "href": "About Me.html",
    "title": "About Me",
    "section": "",
    "text": "Hello Message\nHello!My name is Lan Cao (GU NetID: lc1311).\n\n\nBackground\nLan Cao comes from China, and she is now studying at the Data Science & Analytics Program at Georgetown University. Lan received her bachelor’s degree from Rutgers University (The State University of New Jersey) in May 2023. During undergraduate studies, Lan majored in Economics and minor in Statistics and Data Science. She worked as a writing tutor at Rutgers Writing Centers for two years and she also served as a writing tutor in the Rutgers EOF program in summer 2022. After graduation from Rutgers, Lan is now working as a part-time writing tutor in a company focusing on education services. Except for tutoring, Lan also has a great passion for cooking, and she is good at making traditional Chinese dishes.\n\n\n\n\n\n\nLan Cao\n\n\n\n\n\n\n\nEducation\n\n2023-2025: Georgetown Univeristy, D.C.\n2020-2023: Rutgers Univeristy, New Brunswick, NJ\n2018-2020: Jilin Univeristy, Changchun, China\n\n\n\nAcademic Interests\n\nData Visualization\nHealth Data Analysis\nNLP"
  },
  {
    "objectID": "Code.html",
    "href": "Code.html",
    "title": "Code",
    "section": "",
    "text": "GitHub"
  },
  {
    "objectID": "Data Exploration.html",
    "href": "Data Exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Exploratory Data Analysis (EDA)\nI have several datasets that will be used for this project, and here I will do EDA mainly for the “students_adaptability_level_online_education” dataset.\n\n\nData Understanding:\n\nData Source: Students’ Adaptability Level Prediction in Online Education using Machine Learning Approaches (DOI: 10.1109/ICCCNT51525.2021.9579741) You can find the raw data here\nData Information: There are 14 features in total and all of them are ‘object’ type.\n\n\n\n\ndata info\n\n\n\n\nDescriptive Statistics:\n\nNotes: Most of the data in this dataset is in categorical form, so we will not try to compute descriptive statistics like mean or median. Instead, we will provide frequency distributions to get a rough sense of how these data are distributed.\nComments: Here we can see that some of the variables are binary variables and most of them contains less than 5 values.\n\n\n\n\nfrequency\n\n\n\n\nData Visualization:\n\nComments: First we do univariate analysis for our dataset. These visualizations provide a good overview of the distribution of different variables in the dataset.\n\nKey Observations:\n\nThere are more boys than girls in the dataset. Most students are in the 21-25 age range, followed by 16-20. There are fewer students in the other age ranges.\nA majority of students are from non-government institutions and most students are not IT students.\nMost students have a proper place to participate in online classes. More students use mobile data compared to Wi-Fi.\nMost students are from families with a mid-level financial condition, followed by poor, and then rich.\n\n\n\n\nUnivariate\n\n\n\n\nCorrelation Analysis:\n\nComments: correlation analysis typically applies to numerical data, but all the variables in our dataset are categorical. For categorical variables, we can use other methods like Heatmaps to show associations or dependencies between variables. The plots below can give us some insights into our variables.\n\nKey Findings:\n\nBoth genders show a similar distribution across different adaptivity levels, with the majority in the moderate adaptivity level.\nUniversity students are more likely to have a high adaptivity level compared to school and college students.\nStudents from mid financial condition families are spread across all adaptivity levels, with the majority in the moderate level. Students from rich families are mostly in the moderate and high adaptivity levels, with very few in the low category.\nStudents using computers tend to have higher adaptivity levels compared to those using mobiles or tablets.\n\n\n\n\nBivariate\n\n\n\n\n\nheatmap\n\n\n\n\nHypothesis Generation:\nNotes: Part of my project is to examine what kind of factors can influence student’s online learning. Therefore, we can generate some hypothesis regarding this topic.\nHypothesis:\n\nGender and Adaptivity Level:\n\nNull Hypothesis: There is no significant difference in adaptivity levels between male and female students.\nAlternative Hypothesis: There is a significant difference in adaptivity levels between male and female students.\n\nFinancial Condition and Adaptivity Level:\n\nNull Hypothesis: The financial condition of a student’s family does not affect their adaptivity level to online education.\nAlternative Hypothesis: The financial condition of a student’s family affects their adaptivity level to online education.\n\n\n\n\nData Grouping and Segmentation:\n\nNotes: Here we group the data by education level (University, College, School) to analyze how adaptivity levels and other factors vary across different education levels.\nFindings: The gender distribution is fairly balanced across all education levels; Students from mid financial conditions are prevalent across all education levels; There is a higher proportion of rich students in the university level compared to school and college.\n\n\n\n\neducation-level\n\n\n\n\nIdentifying Outliers:\n\nNotes: Outlier detection is typically applied to numerical data, where statistical methods can be used to identify values that deviate significantly from the rest of the data. However, the dataset does not contain any numerical columns, so our approach to outlier detection will need to be adjusted. We can still investigate the distribution of values in the categorical columns to identify any unusual or unexpected patterns that might indicate data quality issues.\nFindings: Based on the unique value counts for each categorical column, there don’t appear to be any blatant anomalies or irregularities in the dataset. The values in each column seem to be consistent and within expected ranges. However, there are a few points worth noting: for example, the “0” value in the “Class Duration” column could potentially be an anomaly or a data entry error, as it’s not clear what “0” hours of class duration would mean in the context of online education. It might be worth investigating these entries further to understand if they are valid or if they represent a data quality issue.\n\n\n\n\nReport and discussion of methods and findings:\n\nThe dataset contains 1205 entries with 14 categorical variables.There are no missing values, and all categorical variables contain expected and consistent values\nWhen delving into individual variables, we observe a gender distribution skewing slightly towards boys, and a prominent presence of students in the 11-25 age range, with the majority being at the school or university level. A significant portion of these students come from non-government institutions and reside in urban areas. Despite the limited number of IT students, the dataset provides a broad overview of the student demographic.\nIn terms of technology access and internet connectivity, a larger number of students have mobile data connections, primarily on 4G networks, and use mobile devices for online education. This is coupled with a general experience of low load-shedding across the board. Focusing on their online education experience, most students have 1-3 hours of online classes, but there is a noticeable lack of engagement with Learning Management Systems.\nWhen examining the adaptability levels in relation to other variables, certain patterns emerge. Higher adaptability is often linked with being an IT student, having stable internet connectivity (preferably wifi), using computers for online education, engaging with longer online classes, and having access to Learning Management Systems. Financial stability also plays a crucial role, with students from more affluent backgrounds exhibiting higher adaptability levels. Conversely, lower adaptability levels are more common among younger students, those from less affluent backgrounds, those experiencing frequent load-shedding, and those relying on mobile devices for online education.\nThese insights underscore the importance of technological access and financial stability in fostering a conducive environment for online education. They highlight potential areas for intervention, such as improving internet connectivity, promoting the use of Learning Management Systems, and offering additional support to students from less privileged backgrounds to enhance their adaptability and overall experience in an online learning setting.\n\n\n\nTools and Software:\nPython: pandas;matplotlib;seaborn"
  },
  {
    "objectID": "slides/slides.html#brienz",
    "href": "slides/slides.html#brienz",
    "title": "Slide",
    "section": "Brienz",
    "text": "Brienz\n\nSwiss image1"
  },
  {
    "objectID": "slides/slides.html#grindelwald",
    "href": "slides/slides.html#grindelwald",
    "title": "Slide",
    "section": "Grindelwald",
    "text": "Grindelwald\n\nSwiss image1"
  },
  {
    "objectID": "slides/slides.html#eiger",
    "href": "slides/slides.html#eiger",
    "title": "Slide",
    "section": "Eiger",
    "text": "Eiger\nSchool anxiety is one of the main problems faced by teachers and school counselors. This problem attracts special attention for teachers - practitioners, as it is a clear sign of school maladaptation in children, which adversely affects all spheres of their life: not only the studies but also communication, both inside and outside the school, health and general level of mental development and psychological well-being. (Kostyunina and Drozdikova-Zaripova 2016)\n\n\n\n\n\n\n\nKostyunina, Nadezhda Yu, and Albina R Drozdikova-Zaripova. 2016. “Adolescents’ School Anxiety Correction by Means of Mandala Art Therapy.” International Journal of Environmental and Science Education 11 (6): 1105–16."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#part1",
    "href": "about.html#part1",
    "title": "About",
    "section": "part1",
    "text": "part1\nI like chocolate."
  },
  {
    "objectID": "about.html#part2",
    "href": "about.html#part2",
    "title": "About",
    "section": "part2",
    "text": "part2\nI like bubble tea."
  },
  {
    "objectID": "about.html#part3",
    "href": "about.html#part3",
    "title": "About",
    "section": "part3",
    "text": "part3\nI like coffee.\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [5, 7, 5, 8, 3]\n\na = plt.plot(x, y)\n\nprint(a)\n\n\n\n\n[&lt;matplotlib.lines.Line2D object at 0x7fdb3940ac50&gt;]"
  },
  {
    "objectID": "Data Cleaning.html",
    "href": "Data Cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This is Just Initial Data Cleaning\n\nI will only focus on the first part and the fourth part because they are pretty messy\nThe second and third part will be cleaned later because they are relatively easy to clean\n\n\n\nFirst Part\n\nClean “virtual_school_2015.csv”\nYou can see and get the row data here\nClean Notes: since “virtual_school_2015.csv” and “virtual_school_2020” are very similar in terms of structure and variables, so here we will just how the cleaning process of “virtual_school_2015.csv” as an demonstration\nClean Plan: “virtual_school_2015.csv” is a large dataset that contains 52 variables and I plan to use some of them, so I will subset the dataset first. And then I will change some quantative variables to be qualitative variables because some data was transformed into numeric values and lose their original meanings. Therefore, I need to bring their original meaning back for future use.\n\nBefore(this is raw data)\n\nWe can see there are a lot of variables there, so I plan to make a subset\n\n\n\n\nvirtual_school_2015\n\n\nDuring Data Cleaning\n\nYou can get the code here to see how the data was gathered and cleaned\nI use tidyverse package in R to do the cleaning\nFirst choose some variables that I want to use to make a subset\n\n\n\n\nsubset of virtual_school_2015\n\n\n\nThen we will filter the missing values. I don’t plan to replace the missing values because the enrollment of students is unique to every school, therefore making it meaningless to replace the missing enrollment value with any other kind of numbers. Before filtering and subsetting, the dataset has 709 rows, 52 columns. After filtering and subsetting, it has 660 rows, 9 columns. We lost around 7% data\nThe we replace the numeric values of “school_level”, “school_type”, “charter”, “magnet” to be qualitative values\n\nAfter(this is a cleaned version)\n\nYou can get the cleaned data here\n\n\n\n\ncleaned_subset of virtual_school_2015\n\n\n\n\nSecond Part\n\nIt will be cleaned later\n\n\n\nThird Part\n\nIt will be cleaned later\n\n\n\nFourth Part\n\nClean text data “education_trend.json”\nYou can get the raw data here\nWe need to vectorize the data first\n\nBefore(this is raw data)\n\n\n\neudcation_trend\n\n\nAfter(this is a cleaned version)\n\nYou can find the cleaned text data here\n\n\n\n\ntransformed_dataframe"
  },
  {
    "objectID": "Feature selection text.html",
    "href": "Feature selection text.html",
    "title": "Feature selection for text data",
    "section": "",
    "text": "Method\n\nData preprocessing involves the conversion of text data into a numerical format, which is essential for training machine learning models like Naive Bayes. This transformation can be achieved using methods like TF-IDF (Term Frequency-Inverse Document Frequency) or Count Vectorization, which help represent the text as numerical features that can be utilized effectively by the Naive Bayes algorithm.\nWe will vectorize the titles using Count Vectorization. When we convert text data into a numerical format using Count Vectorization, each unique word or token in the text becomes a feature. The value of the feature is the count of how many times that word appears in the document. In our case, the document specifically refers to titles of the news paper. So if our dataset has 10,000 unique words across all the titles, we have 10,000 features.\nFeature selection is a technique used to enhance a model’s performance and mitigate overfitting by picking out the most pertinent features or words from a larger pool. This process involves reducing the total number of unique words to a smaller, more relevant subset that can effectively predict the article’s topic or class labels.\nWe will apply feature selection using the chi-squared test to reduce the number of features to the most informative ones. In text data, there can be a vast number of features (every unique word can be a feature), many of which may not be useful for making predictions. The chi-squared test is a way to handle this high dimensionality by filtering out the noise and keeping only those features that are statistically significant.\nBelow is part of the selected features (Top 20)\n\n\n\n\nfeature selection"
  },
  {
    "objectID": "Dimensionality Reduction.html",
    "href": "Dimensionality Reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Objectives: The objective of this project is to explore and demonstrate the effectiveness of Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) in reducing the dimensionality of complex, multimodal data while preserving essential information and enhancing data visualization. This study aims to provide a deeper understanding of these techniques in handling and visualizing high-dimensional datasets, offering insights into their practical applications and effectiveness.\nDataset Selection: The dataset chosen for this project is “Students’ Adaptability to Online Learning Environments.” It encompasses a variety of attributes, such as gender, institution type, IT student status, location, load-shedding, internet type, LMS usage, age groups, education level, financial condition, network type, class duration, device type, and adaptivity level. You can find the raw data and also the modified data In the previous section, we applied One-Hot Encoding to transform our dataset, which was entirely categorical, into a numeric format. Therefore, we will use the modified dataset for this project.\nTools and Libraries: The project will employ Python due to its extensive support for data analysis and visualization. Key libraries include: Pandas;NumPy;Scikit-learn;Matplotlib;Seaborn."
  },
  {
    "objectID": "Dimensionality Reduction.html#dimensionality-reduction-with-pca",
    "href": "Dimensionality Reduction.html#dimensionality-reduction-with-pca",
    "title": "Dimensionality Reduction",
    "section": "Dimensionality Reduction with PCA:",
    "text": "Dimensionality Reduction with PCA:\n\nPreprocess: PCA is sensitive to the scale of the features and features with larger scales can disproportionately influence the result, leading to misleading principal components. Therefore, we use StandardScaler from scikit-learn to standardize our data first. Standardizing ensures that each feature contributes equally to the result, making the PCA more reliable and interpretable.\nOptimal number of principal components: We will use a scree plot and cumulative explained variance ratio to identify the optimal number of principal components that capture most of the variance in the data. A scree plot is a simple line segment plot that is commonly used in Principal Component Analysis (PCA) to help determine the number of principal components to retain.\n\n\n\n\nscree plot and cumulative explained variance ratio plot\n\n\nTo determine the optimal number of principal components, we typically search for a leveling-off point on the cumulative explained variance plot. This point suggests that additional components contribute little to explaining the dataset’s variance. Aiming to maintain a significant proportion of the variance, such as 95%, we can identify the required number of principal components. In your dataset, this approach indicates that 17 principal components are needed to retain at least 95% of the total variance. By reducing the dataset to these 17 dimensions, we effectively preserve most of the original data’s information, striking a balance between lowering dimensionality and maintaining key data characteristics.\n\nVisualize the reduced-dimensional space:\n\nThe visualization below represents our data projected onto the first two principal components (PC1 and PC2) after reducing the dataset to 17 dimensions using PCA.\nThe PCA plot displays data distribution along the first two principal components. The visible, albeit not sharp, groupings suggest a somewhat overlapping distribution of adaptivity levels in these dimensions. This overlap implies that PC1 and PC2 alone might not distinctly separate adaptivity levels, hinting at the importance of the other components for capturing finer details. Regarding the color scale, while some color concentration in certain areas could signal distinct adaptivity level characteristics, the mixed color distribution in this plot points to a complex interaction between features and adaptivity levels, potentially necessitating more dimensions for clearer differentiation.\n\n\n\nplot of first two principal components\n\n\nBiplot is used in principal component analysis (PCA) to visualize both the principal component scores and the principal component loadings. \nIn the biplot, each vector corresponds to a dataset feature, with longer vectors indicating a stronger influence on the principal components’ variability. Vectors in similar directions suggest positive correlations between features, while opposite directions imply negative correlations.\nFor instance, the vectors for ‘Institution Type’ and ‘Internet Type’ are nearly orthogonal, suggesting these features independently contribute to the dataset’s variance. Conversely, ‘Load-shedding’ and ‘Class Duration’ vectors point in similar directions, indicating a positive correlation, possibly implying that load-shedding impacts class duration.\n\n\n\nbipplot"
  },
  {
    "objectID": "Dimensionality Reduction.html#dimensionality-reduction-with-t-sne",
    "href": "Dimensionality Reduction.html#dimensionality-reduction-with-t-sne",
    "title": "Dimensionality Reduction",
    "section": "Dimensionality Reduction with t-SNE:",
    "text": "Dimensionality Reduction with t-SNE:\nPreprocess: Just like what we do for PCA, we should standardize our data first.\nExplore different perplexity values:\nWith a perplexity of 5, the data points are scattered too uniformly, suggesting it’s too low to discern any meaningful clusters, as it might capture more noise than structure. A perplexity of 20 shows emerging clusters with reduced overlap, yet the data structure isn’t entirely clear. At 30, clusters become more distinct, indicating a better capture of data structure with increased perplexity. Perplexity at 40 reveals even clearer, well-separated clusters, hinting it might be near optimal for the dataset. However, at 50, while clusters remain distinct, they are less defined compared to 40, suggesting that this value may be slightly too high as it begins to merge different groups. Therefore, a perplexity value between 30 and 40 might be appropriate for our dataset.\n\n\n\ndifferent perplexity values\n\n\nVisualization of the t-SNE output\nThe t-SNE visualization indicates distinct groupings corresponding to various adaptivity levels—high, medium, and low. The algorithm seems to be successful in delineating these groups, particularly distinguishing the high adaptivity level with notable clarity. Yet, the transition between low and medium adaptivity levels is less defined, with some overlap present, suggesting a more nuanced or complex relationship between these categories. Outlying data points stand apart from the primary clusters, potentially representing atypical cases or unique data profiles within the broader dataset.\n\n\n\ndifferent perplexity values"
  },
  {
    "objectID": "Dimensionality Reduction.html#evaluation-and-comparison",
    "href": "Dimensionality Reduction.html#evaluation-and-comparison",
    "title": "Dimensionality Reduction",
    "section": "Evaluation and Comparison:",
    "text": "Evaluation and Comparison:\n\nPreserving data structure and information: PCA is a linear method that focuses on keeping significant data features by maximizing variance and projecting data onto high-variance axes, but it might miss complex, nonlinear data relationships. In contrast, t-SNE, a nonlinear technique, excels in highlighting local structures and clusters by unfolding complex patterns, though it may not maintain the overall data structure as effectively as PCA.\nVisualization Capabilities: PCA is adept at capturing the most significant variance in high-dimensional data, projecting it to lower dimensions to visualize overarching patterns and global relationships efficiently. Conversely, t-SNE excels at delineating local clusters and complex structures, presenting a detailed picture of data relationships on a smaller scale that PCA may overlook due to its linear approach. While PCA gives a broad overview with computational ease, t-SNE digs deeper into the data’s fine details, demanding more computational resources.\nDiscussion of the trade-offs: PCA is optimal for quick, high-level data structure analysis and when feature contribution is crucial, as it efficiently handles large datasets and retains global relationships. In contrast, t-SNE is superior for uncovering detailed local groupings and nonlinear patterns in smaller datasets, but it’s computationally heavier and less interpretable. Essentially, PCA is best for dimensionality reduction and overview insights, while t-SNE excels in cluster visualization and data exploration.\n\nNote: Documentation is facilitated by ChatGpt"
  },
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "The feature data X in this dataset encompasses a variety of attributes that describe students in educational settings. This includes demographics (such as gender and age), educational details (like institution type and education level), and technological aspects (including internet and device type), along with factors like financial status and location. Predominantly categorical, these features vary from binary (e.g., gender) to multi-category (e.g., financial condition). The main aim of the clustering analysis is to categorize students into distinct groups based on these attributes to identify common patterns and understand how various factors, from socio-economic background to technology access, interplay and impact students’ adaptability and educational experiences."
  },
  {
    "objectID": "Clustering.html#data-selection",
    "href": "Clustering.html#data-selection",
    "title": "Clustering",
    "section": "Data Selection",
    "text": "Data Selection\n\nThe ‘Adaptivity Level’ is the label, so it would be excluded from the clustering analysis.\n\n# Removing the label column for clustering\ndf_clustering = df.drop(['Adaptivity Level'], axis=1)\n# Standardizing the data \nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df_clustering)"
  },
  {
    "objectID": "Clustering.html#hyper-parameter-tuning",
    "href": "Clustering.html#hyper-parameter-tuning",
    "title": "Clustering",
    "section": "Hyper-parameter tuning",
    "text": "Hyper-parameter tuning\n\nK-Means:\n# Elbow Method\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, n_init= 10)\n    kmeans.fit(df_clustering)\n    wcss.append(kmeans.inertia_)\n\n# Plotting the Elbow Method\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 11), wcss, marker='o', linestyle='--')\nplt.title('Elbow Method for Optimal k')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.xticks(range(1, 11))\nplt.grid(True)\nplt.show()\n\n\n\nElbow\n\n\nsilhouette_scores = []\nfor i in range(2, 11):\n    kmeans = KMeans(n_clusters=i, n_init= 10)\n    kmeans.fit(df_clustering)\n    score = silhouette_score(df_clustering, kmeans.labels_)\n    silhouette_scores.append(score)\n\n# Plotting the Silhouette Scores\nplt.figure(figsize=(11, 7))\nplt.plot(range(2, 11), silhouette_scores, marker='X', linestyle='--')\nplt.title('Silhouette Scores for Different Numbers of Clusters')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')\nplt.xticks(range(2, 11))\nplt.grid(True)\nplt.show()\n\n\n\nSilhouette-score\n\n\nThe Elbow Method suggested optimal clusters at 3 or 4 due to a smaller decrease in WCSS beyond this point. The highest silhouette scores indicated better clustering with 4 to 6 clusters. As silhouette scores directly measure how well data points fit within their cluster compared to others, they’re often more reliable. Therefore, a cluster size of 4 might be optimal for our dataset.\n\n\nDBSCAN:\n# Exploring different distance metrics for DBSCAN\ndistance_metrics = ['euclidean', 'manhattan', 'chebyshev']\neps_values = np.arange(0.1, 2.0, 0.1)\nmin_samples_values = range(2, 20)\n\nresults = []\nfor metric in distance_metrics:\n    for eps in eps_values:\n        for min_samples in min_samples_values:\n            dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)\n            clusters = dbscan.fit_predict(df_scaled)\n\n            # Only compute silhouette score if more than 1 cluster and less than n-1 noise points\n            if len(np.unique(clusters)) &gt; 1 and len(np.unique(clusters)) &lt; len(df_scaled) - 1:\n                silhouette_avg = silhouette_score(df_scaled, clusters)\n                results.append((metric, eps, min_samples, silhouette_avg))\n\nresults_sorted = sorted(results, key=lambda x: x[3], reverse=True)\n\n# Displaying the top 5 results\nprint(results_sorted[:5])\n\n# 3D Visualization \n\n# Extract the eps, min_samples, and silhouette score for plotting\neps_list = [result[1] for result in results_sorted]\nmin_samples_list = [result[2] for result in results_sorted]\nsilhouette_scores = [result[3] for result in results_sorted]\n\n\nfig = plt.figure(figsize=(20, 10))\nax = fig.add_subplot(111, projection='3d')\n\nscat = ax.scatter(eps_list, min_samples_list, silhouette_scores, c=silhouette_scores, cmap='viridis')\n\ncbar = fig.colorbar(scat, shrink=0.5, aspect=5)\ncbar.set_label('Silhouette Score')\nax.set_xlabel('Epsilon (eps)')\nax.set_ylabel('Minimum Samples (min_samples)')\nax.set_zlabel('Silhouette Score')\nax.set_title('3D Plot of DBSCAN Hyperparameters Tuning')\n\nplt.show()\n\n\n\nDBSCAN\n\n\nFrom the plot, we can see that the optimal silhouette scores are generally associated with smaller values of the epsilon parameter and specific values for the minimum number of samples. When epsilon is set too high, it results in a silhouette score decrease, suggesting that the clusters are not well-defined and may be overlapping due to an overly generous neighborhood size. Conversely, setting the minimum number of samples too low appears to detrimentally affect the silhouette score, possibly because it allows more noise to be classified as part of clusters. \nThe best settings for eps and min_samples are those that give the highest silhouette score, indicating clear, tight clusters. The brightest yellow dots on the plot show where these scores are highest. Using these points, we can pinpoint the most effective eps and min_samples values. The Euclidean metric performs well here, likely because the data forms dense clusters with space around them, which Euclidean distance measures effectively.\n\n\nHierarchical Clustering:\n# Hierarchical Clustering\n# Parameters for tuning\nn_clusters_options = range(2, 11)  # Exploring number of clusters from 2 to 10\nlinkage_options = ['ward', 'complete', 'average']  # Different linkage criteria\n\nsilhouette_scores = np.zeros((len(n_clusters_options), len(linkage_options)))\n\nfor i, n_clusters in enumerate(n_clusters_options):\n    for j, linkage in enumerate(linkage_options):\n        clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n        cluster_labels = clustering.fit_predict(df_scaled)\n        score = silhouette_score(df_scaled, cluster_labels)\n        silhouette_scores[i, j] = score\n\nplt.figure(figsize=(12, 8))\nfor i, linkage in enumerate(linkage_options):\n    plt.plot(n_clusters_options, silhouette_scores[:, i], label=f'Linkage: {linkage}')\n\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Scores for Different Linkage Methods')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nIn the context of Hierarchical Clustering, we try different linkage criteria along with various numbers of clusters to see how silhouette_scores vary for different combinations.\nFrom the plot below, we can see that each linkage method suggests a different ideal count of clusters. Ward’s method suggests fewer clusters work best, while complete linkage points to three clusters as the optimal solution. Average linkage, on the other hand, shows that a number of clusters between 2 to 6 may be suitable. Based on our earlier analysis and understanding of our dataset, Ward’s method seems to be the more favorable choice.\n\n\n\n\nHierarchical Clustering"
  },
  {
    "objectID": "Clustering.html#note-you-can-find-all-the-codes-for-hyper-parameter-tuning-and-final-clustering-results-in-the-clustering-folder-here",
    "href": "Clustering.html#note-you-can-find-all-the-codes-for-hyper-parameter-tuning-and-final-clustering-results-in-the-clustering-folder-here",
    "title": "Clustering",
    "section": "Note: You can find all the codes for Hyper-parameter tuning and final clustering results in the “clustering” folder here",
    "text": "Note: You can find all the codes for Hyper-parameter tuning and final clustering results in the “clustering” folder here\n\nFor k-means method, the optimal k value is 4. And below is the result with the optimal k values. We use PCA to reduce the data to be two dimensions.\nThe blue and green groups have an area where they mix, showing they have some things in common or the line between them isn’t very clear. The purple and yellow groups stand out more from the others, with less mixing with the blue and green groups. There are a few spots that are away from the main part of their groups. For example, some spots in the purple group are far from the middle of the group, and these might be seen as unusual. But, using PCA (a method to simplify data) can sometimes make it harder to understand these groups based on the original details.\n\n\n\n\nkmeans final result\n\n\n\nThe DBSCAN clustering outcome following PCA reduction shows clear clusters; however, these groupings don’t align well with the expected patterns of our dataset.\nSince PCA transformation is linear and focuses on preserving global structures with high variance, which might not always align with the local neighborhood relationships that DBSCAN exploits. Therefore, we should other dimensionality reduction method like t-SNE.\n\n\n\n\nDBSCAN final result-PCA\n\n\n\nThe t-SNE method has made the clusters in the plot clearer. We can see distinct groups spread out on the plot. Some groups are close together, while others are looser. DBSCAN’s detection of groups with varying tightness shows it can handle complicated data well. Red stars mark the “noise” — these points don’t belong to any group. Since these noise points are all over the place, it means there are spots in the data that are not dense enough for DBSCAN to form groups with the settings used.\n\n\n\n\nDBSCAN final result- t-SNE\n\n\n\nWe will try both PCA and t-SNE for the Hierarchical Clustering.\nThe plot created using t-SNE shows data points organized into two distinct groups, differentiated by the colors purple and yellow. This demonstrates that Agglomerative Clustering has successfully divided the data into two separate categories. The application of t-SNE for reducing the data’s dimensions before clustering has proven beneficial, as t-SNE is designed to maintain the local relationships of data points, which seems to have revealed the inherent groupings within the data effectively.\n\n\n\n\nHierarchical Clustering(t-SNE)\n\n\n\nImplemented PCA reduction, we can visualize our final results for Hierarchical Clustering from the plot below. We can see that there are two clear clusters indicated by two colors, blue and red. The blue cluster appears more dispersed than the red cluster, which is more densely packed. Overall, it appears that PCA has been a more effective step for dimensionality reduction prior to clustering, enabling Agglomerative Clustering to identify two principal groups within the dataset.\n\n\n\n\nHierarchical Clustering(PCA)"
  },
  {
    "objectID": "Clustering.html#note",
    "href": "Clustering.html#note",
    "title": "Clustering",
    "section": "Note",
    "text": "Note\n\nYou can find all the codes for Hyper-parameter tuning and final clustering results in the “clustering” folder here\nFor k-means method, the optimal k value is 4. And below is the result with the optimal k values. We use PCA to reduce the data to be two dimensions.\nThe blue and green groups have an area where they mix, showing they have some things in common or the line between them isn’t very clear. The purple and yellow groups stand out more from the others, with less mixing with the blue and green groups. There are a few spots that are away from the main part of their groups. For example, some spots in the purple group are far from the middle of the group, and these might be seen as unusual. But, using PCA (a method to simplify data) can sometimes make it harder to understand these groups based on the original details.\n\n\n\n\nkmeans final result\n\n\n\nThe DBSCAN clustering outcome following PCA reduction shows clear clusters; however, these groupings don’t align well with the expected patterns of our dataset.\nSince PCA transformation is linear and focuses on preserving global structures with high variance, which might not always align with the local neighborhood relationships that DBSCAN exploits. Therefore, we should other dimensionality reduction method like t-SNE.\n\n\n\n\nDBSCAN final result-PCA\n\n\n\nThe t-SNE method has made the clusters in the plot clearer. We can see distinct groups spread out on the plot. Some groups are close together, while others are looser. DBSCAN’s detection of groups with varying tightness shows it can handle complicated data well. Red stars mark the “noise” — these points don’t belong to any group. Since these noise points are all over the place, it means there are spots in the data that are not dense enough for DBSCAN to form groups with the settings used.\n\n\n\n\nDBSCAN final result- t-SNE\n\n\n\nWe will try both PCA and t-SNE for the Hierarchical Clustering.\nThe plot created using t-SNE shows data points organized into two distinct groups, differentiated by the colors purple and yellow. This demonstrates that Agglomerative Clustering has successfully divided the data into two separate categories. The application of t-SNE for reducing the data’s dimensions before clustering has proven beneficial, as t-SNE is designed to maintain the local relationships of data points, which seems to have revealed the inherent groupings within the data effectively.\n\n\n\n\nHierarchical Clustering(t-SNE)\n\n\n\nImplemented PCA reduction, we can visualize our final results for Hierarchical Clustering from the plot below. We can see that there are two clear clusters indicated by two colors, blue and red. The blue cluster appears more dispersed than the red cluster, which is more densely packed. Overall, it appears that PCA has been a more effective step for dimensionality reduction prior to clustering, enabling Agglomerative Clustering to identify two principal groups within the dataset.\n\n\n\n\nHierarchical Clustering(PCA)"
  }
]